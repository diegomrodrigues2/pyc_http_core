# Plano Técnico Detalhado: Implementação HTTP Core (Etapa 1) com Loop de Eventos em C

## Visão Geral e Objetivos

Nesta primeira etapa, vamos construir a base de um cliente HTTP/1.1 de alto desempenho, substituindo completamente o loop de eventos padrão (como `asyncio`) por um loop customizado escrito em C (via Cython) utilizando **epoll** no Linux. O objetivo principal é reduzir ao mínimo a latência e a sobrecarga de agendadores em Python, permitindo gerenciar **múltiplas conexões concorrentes** de forma eficiente. Essa arquitetura servirá como o "motor" subjacente de transporte HTTP, seguindo a filosofia de **fazer uma coisa bem feita**. Em resumo, os focos de design são:

* **Loop de Eventos em Baixo Nível (C + epoll):** Implementar um loop de I/O em C que interaja com a camada Python, capaz de aguardar eventos de leitura/escrita em múltiplos sockets via epoll com sobrecarga mínima. Isso substitui loops em Python e elimina o overhead associado (como o gerenciamento O(n) de `select`/`poll` em loops Python). Essa decisão é fundamentada pelo fato de que o epoll pode escalar de forma quase constante mesmo com muitos descritores monitorados (O(1) por evento pronto, contra O(n) de abordagens tradicionais).

* **Conexões HTTP/1.1 Persistentes:** Desenvolver a classe `HTTP11Connection` que gerencia um único socket e fala o protocolo HTTP/1.1. Ela deve oferecer **streaming eficiente** de dados (enviando e recebendo em pedaços, sem carregar grandes volumes na memória) e suportar conexões persistentes (*keep-alive*) para reutilização. Internamente, usaremos a biblioteca `h11` para parsing e serialização do HTTP, mantendo a lógica de protocolo confiável e testada.

* **Modelo Imutável de Request/Response:** Como no projeto `httpcore` original, definiremos classes de alto nível `Request` e `Response` imutáveis, que servem apenas como contêineres de dados (método, URL, cabeçalhos, corpo) sem lógica de I/O. Isso torna o comportamento mais previsível e *thread-safe*, pois uma vez criado um objeto de requisição ou resposta, ele não muda mais, evitando condições de corrida em cenários concorrentes.

Em conjunto, esses componentes formarão um alicerce robusto para evoluções futuras, como suporte a HTTP/2 e WebSockets, mantendo o desempenho e a modularidade. A seguir detalhamos a estrutura do código e como cada parte será implementada.

## Estrutura Modular do Código (Python e Cython)

Para manter o projeto organizado e facilitar evoluções, dividiremos o código em módulos claros, separando as preocupações de protocolo HTTP de detalhes de rede e do loop de eventos:

* **`http_primitives.py`:** Contém as classes de dados imutáveis `Request` e `Response`. Cada `Request` terá campos como método (bytes), URL (tupla de componentes) e cabeçalhos, além de um `stream` representando o corpo da requisição (como um `AsyncIterable` de bytes). A classe `Response` conterá status code, cabeçalhos e um `stream` assíncrono para ler o corpo incrementalmente. Essas classes **não realizam I/O**; são apenas modelos de dados. A imutabilidade garante que, após criação, não haja modificações internas – qualquer alteração (por exemplo, seguir um redirect ajustando a URL) deve gerar um novo objeto. Isso simplifica o raciocínio em concorrência, pois diferentes tarefas não compartilham objetos mutáveis.

* **`http11.py`:** Implementa a classe central `HTTP11Connection`, responsável por gerenciar a comunicação HTTP/1.1 sobre uma conexão TCP. Ela recebe em seu construtor um `NetworkStream` (descrita adiante) já conectado a um servidor de destino (origin – esquema/host/porta). Internamente, mantém:

  * Um estado do protocolo HTTP usando `h11.Connection` (configurado como cliente) para serializar requisições e parsear respostas.
  * Um estado interno da conexão (`NEW`, `ACTIVE`, `IDLE`, `CLOSED`), com um mecanismo de trava (lock) para garantir que apenas uma requisição por vez seja executada por conexão.
  * Métodos assíncronos como `handle_request(request: Request) -> Response` que orquestram o envio da requisição e a recepção da resposta, utilizando métodos auxiliares: `_send_request`, `_receive_response`, e gerenciamento de corpo via `_receive_body_chunk` (ver seção de Streaming abaixo).
  * A lógica de **envio** faz uso do `h11` para obter os bytes correspondentes a cada parte da requisição HTTP: primeiro envia o cabeçalho da requisição (`h11.Request`), depois envia os dados do corpo em chunks (`h11.Data`) iterando sobre `request.stream`, e finaliza com `h11.EndOfMessage`. Cada pacote de bytes produzido por `h11` é escrito no socket via `NetworkStream.write`.
  * A lógica de **recepção** lê bytes do socket (`NetworkStream.read`) e os alimenta no parser `h11` até que se obtenha um evento completo de resposta (`h11.Response`) com status e cabeçalhos. A seguir, cria-se um objeto `Response` com um stream para o corpo (um `ResponseStream` atrelado à conexão). A conexão permanece em estado `ACTIVE` durante toda a requisição e só volta a `IDLE` ou `CLOSED` ao final do corpo (ver Keep-Alive).
  * Métodos como `_response_closed` que, ao final da leitura do corpo, verificam o estado do `h11` e decidem se a conexão pode ser reutilizada ou deve ser fechada.

  Esse módulo abstrai a complexidade do protocolo HTTP/1.1, delegando parsing/formatos ao `h11` (evitando reinventar a roda), e garantindo que a conexão seja usada corretamente (uma requisição por vez). Ele não conhece detalhes de *como* o loop de eventos entrega bytes – ele apenas chama `await stream.read()` ou `await stream.write()` e confia que a infraestrutura de rede/cuidados de concorrência sejam tratados pelo backend.

* **`streams.py`:** Define classes para streaming dos corpos de requisição e resposta de forma incremental. Em especial, a classe `ResponseStream` implementa `__aiter__` para permitir iterar assíncronamente pelos chunks de resposta conforme eles chegam. O `ResponseStream` mantém referência à `HTTP11Connection` e em cada iteração solicita à conexão o próximo pedaço de corpo via `_receive_body_chunk()`. Esse método da conexão usa o `h11` para obter eventos de dados (`h11.Data`) já parseados; se o parser indicar que precisa de mais bytes (`NEED_DATA`), ele realiza `await self._stream.read()` para pegar mais dados do socket. Assim, o consumo do iterador da resposta é que impulsiona a leitura do socket – uma forma de **backpressure natural**: só lemos mais do socket quando o consumidor está pronto para processar mais bytes. O `ResponseStream` ao finalizar (quando encontra `h11.EndOfMessage` indicando fim do corpo) chama `_response_closed()` na conexão para atualizar seu estado. Também oferece um método `aread()` para ler todo o corpo de uma vez (juntando os chunks) e `aclose()` para descartar o restante do corpo caso o usuário queira encerrar a leitura antes do fim. Essa abstração de stream garante que mesmo respostas enormes ou de duração indefinida (p.ex. streaming) possam ser tratadas sem esgotar memória, e que a conexão retorne ao pool apropriadamente ao término do uso.

* **`network_backend.pyx` (ou módulo equivalente em Cython):** Este módulo implementará o **loop de eventos em C** e as primitivas de I/O de rede. Ele expõe para o lado Python uma classe (por exemplo, `EpollEventLoop`) e uma abstração de socket (por exemplo, `NetworkStream`) compatível com as necessidades do `HTTP11Connection`. Aqui concentramos a lógica de baixo nível:

  * **Loop de Eventos (epoll):** Uma classe gerencia a criação do epoll (via `epoll_create` do Linux) e mantém as estruturas para monitorar múltiplos descritores de arquivo (sockets). Provê métodos como `register(fd, eventos, callback)` e `unregister(fd)` para controle dos descritores interessados. O loop terá um método principal (`run()` ou `run_forever()`) que entra em loop aguardando eventos do epoll e despachando-os para callbacks/tarefas correspondentes.
  * **NetworkStream:** Uma classe (possivelmente uma extensão Cython `cdef class` para eficiência) que encapsula um socket não-bloqueante. Ela implementa métodos assíncronos `read()` e `write()` que serão chamados pelo código Python de nível superior. Internamente, essas operações interagirão com o loop de eventos para aguardar disponibilidade:

    * `read(max_bytes)` tenta ler até `max_bytes` bytes do socket. Se os dados estiverem imediatamente disponíveis no buffer do kernel, retorna-os diretamente. Se o socket não tiver dados no momento (causando EWOULDBLOCK/EAGAIN), então **registra interesse** em dados de leitura no epoll e suspende a coroutine até que o loop indique que o socket está pronto para ler. Ao ser retomada (evento recebido), a operação `read` continua, agora conseguindo obter pelo menos um byte. Se o socket retornar `0` bytes, isso indica fechamento pelo lado remoto – a função então retorna `None` ou lança uma exceção de fim de stream para indicar que a conexão fechou.
    * `write(data)` envia bytes pelo socket. Se o kernel aceitar todos os bytes de uma vez (caso comum para dados não muito grandes ou buffer disponível), a chamada completa rapidamente. Se apenas uma parte pôde ser escrita (situação de buffer TCP cheio, retornando um valor menor ou EAGAIN), então o `NetworkStream` deve **guardar o restante dos dados a enviar** e registrar interesse em evento de possibilidade de escrita no epoll. A coroutine será suspensa até o socket ficar escrevível (evento EPOLLOUT), então retoma e continua enviando o restante. Esse envio em partes garante que não bloquearemos o loop.
    * `close()` fecha o socket e remove seu FD do epoll, limpando qualquer estado interno. Será chamado quando a conexão for descartada (estado CLOSED).
  * **Integração com a camada Python:** O backend fornece funções como `connect_tcp(host, port) -> NetworkStream` para abrir um socket TCP (não-bloqueante) e conectá-lo, possivelmente também de forma assíncrona (já que conectar pode demorar). Uma vez conectado (ou negociado TLS na etapa futura), retorna um `NetworkStream` pronto para uso. Este objeto é então passado para `HTTP11Connection` para realizar requisições.
  * Detalhes de implementação do *loop*:

    * O loop em C ficará ativo esperando eventos do kernel. Quando um ou mais sockets ficarem prontos, o epoll retorna uma lista de eventos prontos (cada um com um fd e flags indicando leitura/escrita/erro). O código C iterará por esses eventos e para cada um determinará qual operação liberar:

      * Por exemplo, mantemos um registro (um dicionário ou arrays paralelos em C) de **qual coroutine/futura está aguardando leitura** em cada FD e qual está aguardando escrita. Assim, se recebemos um evento de leitura em um dado fd, encontramos a coroutine/future associada e sinalizamos para acordá-la (no caso de uma Future, marcamos o resultado; no caso de segurar referência direta à coroutine, poderíamos re-agendá-la). O mesmo para escrita.
      * Caso haja um erro (EPOLLERR) ou fechamento (EPOLLHUP) sinalizado, o loop deve notificar a coroutine correspondente com uma exceção indicando erro de conexão ou fim de stream.
    * O loop também precisará gerenciar um conjunto de **tarefas prontas** para executar: isto é, corrotinas que não estão esperando I/O (por exemplo, recém-desbloqueadas pelo epoll ou iniciadas pelo usuário). Em cada iteração do loop, após processar eventos de I/O, ele irá agendar as tarefas prontas para rodar. As tarefas são executadas cooperativamente – cada uma roda até fazer um `await` em algo (por exemplo, I/O ou um sleep futuramente) e então cede de volta ao loop.
    * Essa lógica espelha a de outros loops assíncronos: *"executar continuamente as corrotinas prontas, registrar novos eventos de espera no sistema operacional, e dormir aguardando eventos; quando um evento ocorre, acordar a tarefa certa"*. Projetos existentes como o **Shrapnel** já mostraram que é viável implementar isso em Cython com epoll, atingindo alta escalabilidade (dezenas de milhares de conexões). Também o **uvloop**, um loop de eventos escrito em Cython usando libuv, demonstra ganhos de performance significativos – em torno de 2x a 4x mais rápido que o loop asyncio padrão – validando a abordagem de mover a espera de I/O para código de baixo nível otimizado.
    * O loop será preferencialmente executado no *mesmo thread* que o código do cliente, de forma semelhante ao asyncio padrão (monothreadado). Podemos fornecer um método como `run(coro)` para iniciar a execução de uma coroutine até completar, ou integrar com o padrão `asyncio.run` (usando nossa implementação como backend default). Isso facilita o uso: o desenvolvedor do cliente de alto nível (como um `httpx` adaptado) pode simplesmente await nas chamadas sem precisar se preocupar com threads extras.
    * **Importante:** se for necessário iniciar o loop em segundo plano (por exemplo, para fornecer uma API síncrona usando um thread de loop), deve-se implementar mecanismos seguros para enviar tarefas de outro thread para o loop (similar a `loop.call_soon_threadsafe` do asyncio). Nesta etapa, podemos manter o modelo simples de loop único no thread principal, e documentar que não é thread-safe acessar o loop de fora dele sem mecanismos apropriados.

Resumidamente, essa separação modular permite evoluir e testar componentes isoladamente. Por exemplo, podemos testar a lógica do `HTTP11Connection` usando um `MockNetworkStream` (que simula leituras/escritas), independentemente do loop real. Da mesma forma, o loop epoll e `NetworkStream` podem ser testados em nível de socket bruto.

## Loop de Eventos em C com epoll (Design e Integração)

Nesta seção, detalhamos como o loop em C baseado em epoll funcionará e se comunicará com a camada Python:

* **Uso do epoll para Múltiplas Conexões:** Ao iniciar a aplicação, cria-se um *epoll instance* único (chamada `epoll_create` no C). Esse objeto de kernel nos permite registrar qualquer número de descritores de socket para monitorar eventos. A grande vantagem é que o *epoll* mantém um contexto interno: não é necessário reespecificar todos os sockets a cada espera, ao contrário do `select`/`poll`. Registramos cada socket conforme é criado (ou quando há algo para aguardar) e o epoll nos retornará **somente os sockets prontos** a cada chamada de wait. Isso torna a espera muito eficiente – o custo de acordar, por exemplo, 1000 sockets ativos simultaneamente é proporcional ao número de sockets prontos, não ao total monitorado.

* **Estrutura de Dados Interna:** Dentro da implementação C do loop, manteremos:

  * Uma estrutura (p.ex. `struct epoll_event events[MAX_EVENTS]`) para receber os eventos do kernel a cada chamada de `epoll_wait`. `MAX_EVENTS` pode ser um valor fixo (p.ex. 100 ou 1000) ou adaptativo, definindo quantos eventos processamos por iteração no máximo.
  * Uma tabela de registro de corrotinas/futures por FD e tipo de evento:

    * Podemos usar dois mapas Python (ou estruturas Cython) para isso: `waiting_read[fd] = future` e `waiting_write[fd] = future` (ou equivalentes) indicando que a determinada future/coroutine está parada aguardando leitura ou escrita naquele socket. Alternativamente, poderíamos guardar callbacks diretos, mas usar uma future permite integrar no modelo `await`.
    * Também uma lista/queue de `ready_tasks` (corrotinas prontas para execução imediata). Essa lista é preenchida quando tasks são liberadas de espera de I/O ou quando adicionamos novas corrotinas via `run()`.

* **Ciclo do Loop:** O método `run_forever()` do loop (ou equivalente) executará repetidamente os seguintes passos:

  1. **Executar Tarefas Prontas:** Para cada tarefa pronta na fila, retomá-la. Isso significa chamar `coro.send(value)` ou criar uma `Task` wrapper que já possui seu contexto. Na prática, podemos implementar nossas próprias `Task` e `Future` simples: uma `Task` guarda a coroutine e seu estado, e no loop chamamos `task.step()` que avança até o próximo await. Quando a coroutine realiza um `await` em nosso `NetworkStream.read` ou similar, internamente isso retornará uma `Future` que indica a suspensão em I/O. Detectamos que a coroutine não terminou (lançou `StopIteration` apenas quando completa) e que retornou uma espera; então registramos essa espera (por exemplo, vemos que é um objeto especial esperando leitura de FD X, então guardamos `waiting_read[X] = future` e armamos o epoll para monitorar `EPOLLIN` em X). A `Task` então fica pendente atrelada àquela future.
  2. **Espera por I/O:** Depois de executar todas as tarefas prontas no momento, chamamos `epoll_wait`. Importante: podemos calcular um `timeout` baseado no próximo timer agendado (se suportarmos timers), ou deixar bloqueio "infinito" (ou até próximo keep-alive timeout) se não houver nenhuma tarefa com deadline. Se não houver nenhuma tarefa pendente nem eventos a esperar, o loop pode terminar. Normalmente, porém, esperamos bloqueado até algum socket ter atividade ou até alguma outra interrupção.

     * Chamamos `epoll_wait(epfd, events, MAX_EVENTS, timeout)` dentro de um bloco liberando o GIL (com `with nogil` em Cython). Isso permite que outros threads Python (se existirem) não fiquem bloqueados por este loop (embora tipicamente não teremos outras threads ativas a não ser possivelmente o main thread de usuário). Quando `epoll_wait` retorna, recuperamos o GIL para manipular os resultados.
  3. **Processar Eventos:** Iteramos sobre cada evento retornado (são no máximo `MAX_EVENTS`). Para cada evento:

     * Identificar o `fd` e o conjunto de flags (por ex., EPOLLIN, EPOLLOUT, EPOLLHUP, EPOLLERR).
     * Se houver `EPOLLIN` e tivermos alguém em `waiting_read[fd]`, significa que a coroutine aguardando leitura agora pode prosseguir. Removemos esse registro e marcamos a futura como concluída (por exemplo, `future.set_result(None)` apenas para acordá-la, ou armazenamos diretamente dados lidos se já lemos aqui). Provavelmente é mais simples sinalizar apenas que está pronto e deixar a coroutine fazer a leitura efetiva quando retomar (garantindo assim que a lógica de leitura/parse continue do ponto certo). Então, colocamos a tarefa correspondente na fila de prontas para a próxima iteração do loop (ou até mesmo imediata).
     * Similarmente, `EPOLLOUT` indica que um socket previamente bloqueado para escrita agora pode aceitar dados. Recuperamos quem estava em `waiting_write[fd]`, marcamos como pronto e o agendamos.
     * `EPOLLHUP` ou `EPOLLERR`: normalmente acompanhados de IN/OUT, mas se aparecem sozinhos indicam que o socket foi fechado ou erro grave. Nesse caso, liberamos tanto leitores quanto escritores que possam estar esperando nesse fd, sinalizando uma exceção (por exemplo, `future.set_exception(BrokenConnectionError)`).
     * **Remoção de registro:** Se o epoll nos informa que um fd fechou ou não será mais usado, devemos chamar `epoll_ctl(DEL)` para removê-lo do conjunto monitorado e fechar o fd conforme apropriado.
  4. **Repetir:** Voltar ao passo 1, executando agora as tasks que acabaram de ser liberadas. Esse ciclo continua até que não haja mais tasks nem fds (no caso de encerrar a aplicação ou loop).

  Essa estratégia assegura que as operações de I/O bloqueantes ocorram **fora do interpretador Python** (durante o `epoll_wait`, o thread fica adormecido no kernel, liberando CPU) e que, quando despertadas, apenas a tarefa relevante é agendada, evitando varreduras lineares de todos os sockets. Conforme mencionado, a complexidade por evento pronto é efetivamente O(1), o que suporta grande escalabilidade.

* **Interação com Corrotinas Python:** Para fazer a ponte entre o mundo C do loop e o mundo Python das `async def` corrotinas, definiremos algumas primitivas:

  * Provavelmente uma classe `Future` simples (em Python ou Cython) com métodos `set_result`/`set_exception` e um `__await__` que permite `await future`. Quando uma coroutine faz `await future`, o nosso loop em C verá que ela cedeu um objeto do tipo Future não resolvido; então o loop sabe que deve estacionar essa task até o Future ser concluído (e registra possivelmente o Future se necessário). No caso específico de espera de I/O, podemos encapsular a espera em um Future: por exemplo, `loop.wait_readable(fd)` poderia retornar um Future que o loop conhece e vai completar quando aquele fd tiver dados. Assim, na implementação de `NetworkStream.read`, podemos fazer `await self._loop.wait_readable(self.fd)` internamente. Esse Future, ao ser criado, já registra `waiting_read[fd] = future` e faz o epoll monitorar `fd` para leitura. A coroutine então pausa nessa linha. Quando o epoll retornar, o loop chamará `future.set_result(None)`, fazendo com que a coroutine retome exatamente após o `await`. Esse padrão segue o modelo do asyncio (Future/Task) porém de forma otimizada e interna.
  * Podemos também implementar a própria classe `Task` (que envolve uma coroutine) para gerenciá-la no loop. Contudo, é possível simplificar usando as corrotinas diretamente, desde que tenhamos como reintroduzi-las no loop (via `coro.send(None)` etc.). Em Cython, manipular corrotinas do Python diretamente é complexo, então possivelmente é mais fácil criar um objeto Task em Python que o loop gerencia.
  * **Locks e sincronização:** Além das I/O futures, precisamos de mecanismos como `Lock` assíncrono (mencionado no `HTTP11Connection` para proteger estado). Implementaremos um `Lock` simples no backend: internamente, se `lock.acquire()` encontrar o lock já tomado, ela registra a coroutine atual numa fila de espera e suspende (similar ao I/O, usando talvez uma Future ou fila interna); quando `lock.release()` é chamado, acorda uma das tasks aguardando. Isso tudo pode ser gerenciado sem envolver threads, usando o loop cooperativamente. Esses locks garantem *task-safety* em recursos compartilhados, mesmo rodando no mesmo thread, ao evitar condições de corrida quando duas corrotinas possam concorrer (por exemplo, duas requisições tentando usar a mesma conexão ao mesmo tempo).

* **Eventos Temporais (Timeouts):** Embora foco seja I/O, vale notar que para recursos como timeouts de keep-alive poderíamos precisar de eventos temporais. O loop pode oferecer função `call_later(delay, callback)` ou similar. Isso exigiria que a cada iteração do loop verificássemos algum min-heap ou lista de horários para executar callbacks vencidos. Nesta fase inicial, podemos não implementar timers complexos, mas *deixar ganchos* para isso. Por exemplo, podemos querer encerrar uma conexão que ficou ociosa por muito tempo: sem timers, podemos simplesmente verificar o tempo ao pegar uma conexão do pool e se passou do tempo, fechá-la. Alternativamente, registrar um timer no loop quando a conexão fica idle, para daqui X segundos fechar. Essa funcionalidade pode ser adicionada depois – mas é importante projetar o loop de forma extensível para acomodar timers sem grandes modificações.

Em resumo, o loop de eventos em C com epoll será o núcleo que **despacha eventos de I/O com latência mínima**. Ele permitirá que as corrotinas do Python (como as implementações de requisição HTTP) rodem de forma cooperativa e sem bloqueios, enquanto a espera por dados no socket acontece eficientemente no kernel. Essa abordagem elimina a sobrecarga de *busy-wait* ou de loops Python verificando sockets – tudo é dirigido por eventos do sistema operacional, o que maximiza performance e escalabilidade. Em testes comparativos, é esperado que essa implementação se equipare ou supere loops existentes – considerando que uvloop (também escrito em Cython sobre libuv) conseguiu melhorar significativamente a performance do asyncio padrão. Nossa solução é até mais *enxuta*, focada apenas em sockets e com menor generalidade, podendo assim ser otimizada sob medida para operações HTTP.

## Interface de NetworkStream (Streams e Sockets Personalizados)

A interface `NetworkStream` faz a ponte entre o loop de eventos e o código de alto nível que precisa ler/escrever dados. Vamos detalhar sua API e como ela é implementada para se adequar à nova arquitetura:

* **Design da Classe `NetworkStream`:** Representa um socket de transporte (normalmente TCP). Provê operações básicas:

  * `async read(max_bytes) -> bytes`: Lê do socket até *max\_bytes* bytes, podendo retornar menos se um chunk chegar (não espera acumular exatamente max\_bytes, retorna o que estiver disponível, mínimo de 1 byte, ou retorna `b""`/`None` em caso de fim).
  * `async write(data: bytes)`: Envia todos os bytes de `data` no socket, de forma assíncrona (retorna quando todos foram efetivamente enviados ou lança exceção se a conexão quebrou).
  * `close()`: Fecha imediatamente o socket (encerra a conexão).
  * (Potencialmente) `start_tls(ssl_context, server_hostname)` no futuro: para promover a conexão para TLS, mas isso pertence a etapas posteriores (HTTPS).

  Essa interface é inspirada no *backend* do `httpcore`, onde há implementações específicas para asyncio, trio, etc., todas apresentando métodos semelhantes de leitura/escrita para serem usadas pelas camadas superiores. Aqui, implementaremos a nossa versão especializada.

* **Implementação da Leitura (`read`):** Esta é a operação crucial de recepção de dados:

  * O socket será configurado como não-bloqueante no momento da conexão.
  * Ao chamar `read(n)`, internamente tentaremos ler do descritor:

    * Usaremos chamada de baixo nível, por exemplo `recv`/`read` do POSIX. Podemos chamar `socket.recv(n)` do Python, mas isso envolve alguma sobrecarga e possíveis cópias. Como estamos em Cython, podemos chamar diretamente a função C `read(fd, buffer, n)` ou `recv(fd, ..., MSG_NOSIGNAL)` etc., alocando um buffer C de tamanho `n` (ou um PyBytes de tamanho n e obtendo seu ponteiro para preencher). Essa chamada pode ser envolvida em `with nogil` para não segurar o GIL durante o IO.
    * Se a chamada `read` retornar um número positivo de bytes `k <= n`, convertemos isso para um objeto `bytes` Python (se já não for) e retornamos imediatamente, **sem esperar pelo tamanho total**. Isso porque no HTTP/1.1 a camada superior (h11) vai chamar repetidamente até consumir tudo necessário. Não queremos atrasar artificialmente – devolvemos o que chegou prontamente.
    * Se a chamada indicar *would block* (errno EAGAIN/EWOULDBLOCK) ou retornar 0 bytes:

      * *0 bytes*: significa fim de stream (o peer fechou a conexão ordenadamente). Nesse caso, lançamos uma exceção do tipo `RemoteProtocolError` ou simplesmente retornamos um indicador especial (talvez `None`) para que o `HTTP11Connection` interprete como conexão fechada e aja conforme (provavelmente fechando e não reutilizando).
      * *EAGAIN (nenhum dado pronto agora)*: Aqui entra a integração com o loop. Não podemos satisfazer a leitura agora, então **registramos** o interesse em leitura:

        * Chamamos algo como `self.loop.wait_readable(self.fd)`. Essa função do loop criará um Future (ou usará um preexistente) e registrará o fd no epoll para evento de leitura. Imediatamente, a coroutine `read` será suspensa (retornando ao loop superior), e o controle volta para o loop de eventos.
        * Quando o epoll notificar que `fd` está legível (EPOLLIN), o loop setará o Future como concluído e agendará a coroutine do `read` de volta.
        * A execução de `read` então retoma após o `await`, e recomeça o processo: tenta ler novamente do socket. Desta vez, espera-se que haja dados disponíveis (caso contrário, estranhamente receberíamos outro EAGAIN e faríamos o ciclo novamente). Assim obtém bytes e retorna.
    * Esse mecanismo implementa de forma eficiente um **read assíncrono orientado a evento**. Note que a lógica de repetição (loop NEED\_DATA do h11) já vai reinvocar `await stream.read()` quantas vezes forem necessárias, então o comportamento final é: a coroutine de alto nível pede mais dados repetidamente até o parser indicar fim da mensagem.
    * **Tamanho do buffer:** normalmente, usaremos um tamanho fixo (por ex 64KiB) para ler ou o `max_bytes` passado. O `HTTP11Connection` no código de exemplo usa 65536 bytes como chunk de leitura. Esse é um bom tamanho para eficiência. Podemos fazê-lo fixo, ignorando o `max_bytes` se quisermos simplificar, já que o parser lida com pacotes parciais tranquilamente. Mas para generalidade, respeitaremos `max_bytes` se fornecido.

* **Implementação da Escrita (`write`):** Escrever em um socket não-bloqueante requer lidar com o caso de buffer cheio:

  * Utilizamos `send`/`write` do socket para despachar os dados. Em Cython, podemos novamente chamar a função C (`send` do sistema ou via `PySocketSock_Send` da C-API) para evitar overhead. Dividiremos a mensagem caso seja muito grande? Em geral, `send` já aceita limitar bytes por chamada. Podemos tentar enviar tudo de uma vez.
  * Se a chamada enviar todos os bytes (retorno = tamanho do data), ótimo – concluímos e retornamos. Provavelmente esse será o caso para requisições usuais (cabeçalhos pequenos, etc).
  * Se enviar parcial (retorno = k < len(data)), então restam `data[k:]` a enviar:

    * Podemos armazenar esse restante em um buffer interno (por exemplo, atributo `_write_buffer` no `NetworkStream`) e registrar interesse em escrita no epoll (`wait_writable(fd)` similar ao acima). Suspendemos a coroutine até o socket sinalizar que espaço no buffer está disponível.
    * Quando acordar (EPOLLOUT recebido), tentamos novamente enviar o restante. Isso pode ocorrer em loop se o socket for muito lento, mas normalmente algumas iterações bastam.
  * Se `send` retornar EAGAIN imediatamente (nenhum byte enviado, possivelmente buffer cheio ou simplesmente não pôde enviar naquele momento), faremos o mesmo registro de espera sem ter enviado nada ainda.
  * Tratar erros: se ocorrer um erro (retorno -1 com errno != EAGAIN), lançaremos exceção apropriada (por ex. BrokenPipe, etc), encerrando a operação.
  * **Flush e Nagle:** Em alguns casos, pode ser interessante desabilitar Nagle (`TCP_NODELAY`) para enviar pacotes pequenos sem atraso, já que nosso objetivo é latência mínima. Essa opção de socket pode ser setada na criação para otimizar requisições interativas (depende do cenário). Menciono aqui pois faz parte de performance.

* **Manutenção de Conexão:** O `NetworkStream` em si não implementa *reconnect* ou pooling – isso caberá ao gerenciador de conexões (ConnectionPool na etapa 2). Contudo, ele deve suportar ser fechado e descartado com segurança. Após um `close()`, quaisquer corrotinas pendentes de leitura/escrita nesse stream devem ser despertadas com exceção (indicando cancelamento). Nosso loop tratará isso quando receber EPOLLHUP/ERR, mas se o fechamento for iniciado pelo cliente, devemos proativamente cancelar eventos associados:

  * Implementaremos `NetworkStream.close()` de forma que chame `epoll_ctl(DEL)` e feche o descritor. Antes de fechar, pode verificar `waiting_read[fd]` e `waiting_write[fd]`: se existir algo, marcar suas futures com exceção (CancelledError, por ex). Depois, libera os recursos.

* **Segurança e Performance da Interface:**

  * Para maximizar performance, faremos essa classe em Cython com atributos cdef (por exemplo, armazenar o int do file descriptor, a referência ao loop, etc.) para acesso rápido. Assim, chamar `self.fd` não passa pelo dicionário de atributos Python, e operações repetitivas dentro de read/write ocorrem em C.
  * I/O em C liberando GIL: tanto a leitura quanto a escrita podem ser envolvidas em blocos `nogil` nas partes bloqueantes. Assim, teoricamente várias threads poderiam esperar I/O simultaneamente. No nosso design não estamos usando multi-thread para I/O, mas liberar GIL aqui permite que se o nosso loop thread está esperando no kernel, outra thread (por exemplo, do usuário fazendo log ou outra tarefa) não fica presa.
  * Não expomos o file descriptor bruto para o usuário Python – isso evita usos indevidos. Todo acesso é mediado pelos métodos seguros. Se realmente precisarmos expor algo (como local/peer address), podemos dar métodos que retornam strings ou tuplas com essas informações.
  * Controle de erros: Qualquer exceção no nível C (por ex, chamada de sistema falhar) deve ser capturada e convertida em exceção Python apropriada, para não "quebrar" o loop. Cython facilita isso detectando retornos -1 e errno, etc.
  * **Testabilidade:** Podemos fornecer uma implementação de `NetworkStream` simulada (mock) para testar `HTTP11Connection` isoladamente. Como a interface é pequena (read/write/close), isso é fácil de mockar.

Com essa interface de `NetworkStream`, o código de alto nível (`HTTP11Connection` e `ResponseStream`) não precisa saber nada sobre epoll ou C – ele apenas faz `await stream.read()` e `await stream.write()` e obtém o comportamento desejado. Internamente, garantimos que isso ocorra de forma **não bloqueante e otimizada**, tirando proveito total do loop em C.

## Implementação de Streaming Incremental (Resposta e Requisição)

O sistema de streaming é fundamental para lidar com grandes cargas ou fluxos contínuos de dados sem consumir muita memória. Já introduzimos partes dele, mas aqui recapitulamos com foco em como o loop epoll facilita o streaming:

* **Requisição (Envio Streaming):** A classe `Request` permite que o corpo (`request.stream`) seja um iterador assíncrono de bytes. Isso significa que podemos, por exemplo, ler um arquivo do disco de forma assíncrona e enviar aos poucos, ou gerar um corpo dinamicamente. Na implementação de `HTTP11Connection._send_request`, vemos que após enviar os cabeçalhos, ele faz:

  ```python
  async for chunk in request.stream:
      event = h11.Data(data=chunk)
      await self._send_event(event)
  ```


  O método `_send_event` basicamente pede ao `h11` os bytes correspondentes e os envia via `NetworkStream.write`. Graças à nossa implementação de `write`, isso pode suspender a coroutine caso o socket não consiga enviar tudo de uma vez (por congestionamento), e retomar quando possível. Assim, mesmo um upload grande (ou uma transmissão lenta) não bloqueará o loop inteiro – apenas aquela tarefa fica aguardando a possibilidade de escrever, enquanto outras conexões podem estar ativas. Ao terminar os chunks, enviamos um `h11.EndOfMessage` para sinalizar fim do request.

* **Resposta (Leitura Streaming):** Quando enviamos a requisição, mudamos o foco para ler a resposta. O método `_receive_response` entra em um loop chamando `h11_connection.next_event()`. Inicialmente ele provavelmente retorna `NEED_DATA`, então fazemos `data = await self._stream.read(65536)` e alimentamos no parser (`h11_connection.receive_data(data)`). Esse loop continua até receber um evento `Response` completo contendo status e cabeçalhos, que então retornamos. Note que, internamente, durante esses `await self._stream.read()`, a coroutine de conexão está liberando o controle para o loop. O loop, por sua vez, espera o epoll indicar que há dados antes de realmente retornar algo. O resultado é que assim que os primeiros bytes da resposta chegam, eles são processados e a aplicação pode começar a agir, em vez de esperar todo o corpo.

  Após obter cabeçalhos, construímos o objeto `Response` com um `ResponseStream` associado. O corpo não é lido totalmente nesse momento – apenas quando o usuário final consumir o `response.stream`. Ao iterar sobre `ResponseStream` (via `async for chunk in response.stream`), o método `ResponseStream.__aiter__` fará:

  ```python
  chunk = await self._connection._receive_body_chunk()
  if chunk is None: break
  yield chunk
  ```

  em loop. `_receive_body_chunk` na conexão funciona similar ao `_receive_response`, chamando `next_event()` no parser: pode devolver diretamente um `h11.Data` (com um chunk do corpo), ou `NEED_DATA` levando a um `await self._stream.read()` para pegar mais bytes e alimentar o parser. Quando encontra `h11.EndOfMessage`, retorna `None` para sinalizar fim. Assim, enquanto há dados no socket, iremos recebê-los de maneira incremental, aproveitando cada notificação do epoll de que novos dados chegaram. Importante: se o servidor envia uma resposta muito longa, poderemos ter dezenas de iterações, mas em nenhuma delas bloquearemos a thread – a espera entre elas é dentro do `await self._stream.read()`, gerenciado pelo loop epoll.

  Ao final da iteração (ou se o consumidor decidir parar antes), o bloco `finally` do `__aiter__` chama `await self._connection._response_closed()`. Esse método verifica o estado interno do parser h11:

  * Se tanto nosso lado quanto o lado do servidor estão em estado `DONE` (ou seja, request/response completamente terminados), significa que a conexão pode ser **reutilizada**. Chamamos `self._h11_state.start_next_cycle()` para resetar o parser para um próximo uso e marcamos `self._state = "IDLE"`. Isso sinaliza para o pool (quando existir) que a conexão está livre para outra requisição.
  * Se, pelo contrário, o parser indicar que a conexão foi fechada (`their_state` é `CLOSED` ou houve algum erro), então partimos para fechar também a conexão (`await self.close()`), marcando estado `CLOSED`. Por exemplo, se o servidor enviou um cabeçalho `Connection: close` ou simplesmente fechou o socket após a resposta, o h11 indicará que não há próximo ciclo. Nesse caso não tentamos reutilizar.

  Esse mecanismo garante o cumprimento do protocolo HTTP/1.1 em termos de *keep-alive*: por padrão, conexões permanecem abertas para reutilização, a não ser que alguma das partes indique fechamento. Nossa implementação, graças ao `h11`, não precisa inspecionar manualmente cabeçalhos – o estado final do parser já reflete se houve um `Connection: close` ou fim de dados inesperado.

* **Efeito do Loop Customizado no Streaming:** Um loop de eventos mais rápido e focado traz benefícios diretos ao streaming:

  * Latência baixa na disponibilização de chunks: assim que um pacote de dados chega no socket, o epoll notifica prontamente e nossa coroutine de resposta retoma e entrega o chunk. Não há varreduras ou delays desnecessários. Isso é importante para cenários de **baixa latência** (e.g., aplicações em tempo real ou requisições com tempos de resposta muito curtos).
  * Menor overhead por chunk: cada ciclo de leitura (chunk) envolve poucas chamadas Python, já que a espera foi tratada em C. Mesmo que uma resposta chegue em centenas de pacotes TCP pequenos, o custo de iterar sobre `ResponseStream` continua baixo, evitando sobrecarregar a CPU apenas com o gerenciamento de eventos.
  * **Backpressure natural:** Como mencionado, o consumo do stream pelo cliente final controla o ritmo de leitura. Se o cliente lê devagar, o nosso loop simplesmente vai acumulando os dados recebidos no buffer kernel (até um limite) – se esse limite atinge, o servidor pode pausar envio via controle de fluxo TCP, equilibrando o sistema. Por outro lado, se o cliente consome rapidamente, nosso loop pega dados assim que chegam. Essa harmonia é conseguida sem esforço extra porque o modelo de async/await + epoll lida bem com esse acoplamento produtor/consumidor.

Em suma, **streaming incremental** está no coração do design de performance. Já na Etapa 1 garantimos que nem requisições com upload grande, nem respostas longas, vão quebrar a eficiência – sempre tratamos em pedaços e de forma concorrente. Isso estabelece uma base sólida para funcionalidade mais avançada (como streaming de resposta de SSE ou download de arquivos enormes), que poderão ser construídas em cima dessa infraestrutura sem mudanças fundamentais.

## Gerenciamento de Keep-Alive e Estados da Conexão

Manter conexões persistentes abertas (keep-alive) é crucial para performance em HTTP/1.1, evitando o custo de estabelecer uma nova conexão TCP para cada requisição. Nosso plano inclui lógica para gerenciar isso cuidadosamente:

* **Estados Internos e Lock:** Conforme citado, `HTTP11Connection` tem `_state` indicando se está livre ou ocupada, e um `_state_lock` para sincronizar acesso. Assim, quando o usuário (ou o pool) quiser usar uma conexão, ele deve primeiro adquirir o lock e verificar se `_state` é `"IDLE"` ou `"NEW"`. Se estiver `"ACTIVE"`, significa que já há uma requisição em andamento ali – não podendo reutilizar agora. Esse mecanismo impede colisão de duas tasks usando a mesma conexão simultaneamente, algo que violaria o protocolo HTTP/1.1 (que não suporta, sem pipelining, duas requisições ao mesmo tempo na mesma conexão).

* **Transição de Estados:** O ciclo típico já descrito em `handle_request` é:

  1. Trancar e marcar estado como `"ACTIVE"` no início de uma requisição.
  2. Enviar requisição e ler resposta.
  3. Quando o corpo da resposta é totalmente lido (ou a leitura interrompida pelo usuário), executar `_response_closed`:

     * Se o `h11` indicar que podemos continuar (ambos lados em DONE), mudar estado para `"IDLE"` e preparar o parser pro próximo request.
     * Se não, estado `"CLOSED"` e fechar socket.
  4. Liberar o `_state_lock` para que outra tarefa possa usar (no caso IDLE, a conexão retorna ao pool disponível; no caso CLOSED, o objeto conexão possivelmente será descartado).

* **Gerenciamento do Keep-Alive no Loop de Eventos:** Quando uma conexão está ociosa aguardando uma próxima requisição, o socket permanece aberto. Nesse estado, não temos nenhuma coroutine bloqueada esperando nesse socket (pois no momento não há nem leitura nem escrita em progresso). Entretanto, convém **continuar monitorando** o socket para certos eventos:

  * Se o servidor decidir fechar a conexão inativa (por política própria ou timeout), nosso epoll receberá um EPOLLHUP/ERR numa conexão teoricamente IDLE. Precisamos detectar isso para marcar a conexão como fechada antes de reutilizar. Portanto, poderemos manter o socket registrado no epoll mesmo quando IDLE, talvez monitorando `EPOLLIN` e `EPOLLERR`. Se chegar qualquer evento inesperado enquanto estamos IDLE:

    * Se for dados (EPOLLIN) sem termos feito uma requisição – isso seria inusitado (servidor enviando algo espontaneamente, o que em HTTP tradicional não ocorre a não ser talvez um *pushed* response ou algo fora do escopo HTTP/1.1 puro). Em geral, ignoraríamos ou fecharíamos.
    * Se for HUP/ERR, entendemos como "servidor fechou a conexão por inatividade", então marcaremos `CLOSED` e limpamos do pool.
  * Alternativamente, para simplificar, poderemos **desregistrar** o socket do epoll quando em IDLE, e apenas registrá-lo novamente quando formos usá-lo (para ler/escrever). Isso evita falsos positivos. O risco é não detectar um fechamento pelo servidor até tentar usar novamente e falhar. Mas isso é tratável: se tentarmos escrever e receber erro, basta descartar conexão. Em muitos clientes HTTP, isso é aceitável (o primeiro request após timeout de keep-alive pode falhar e o pool reabre outro). De qualquer forma, podemos documentar que conexões podem expirar silenciosamente.

* **Política de Idle Timeout:** Em um contexto real, não queremos segurar conexões ociosas para sempre (consome recursos no servidor e no cliente). Uma prática comum é definir um **keep-alive timeout** (ex.: 30 segundos). Implementaremos essa política provavelmente no nível do pool (que regularmente fecha conexões antigas). Mas já na arquitetura do loop, podemos deixar abertura para isso: por exemplo, o pool pode chamar `stream.close()` em conexões que estejam IDLE por mais que X segundos. Para suportar isso, podemos guardar na `HTTP11Connection` um timestamp de quando ela ficou ociosa e expô-lo ao pool via método `has_expired(timeout)`. Assim, o pool ao gerenciar seu ciclo verifica e fecha conforme necessidade.

* **Reaproveitamento e Concorrência:** Com essas regras, conseguimos reutilizar conexões de forma segura. O acesso ao pool será protegido por locks também (para múltiplas tasks pegando conexões para hosts iguais). O `HTTP11Connection` isoladamente, porém, graças ao lock interno, já se garante *task-safe*. Note que isso não conflita com nossa abordagem single-thread: as tasks não executam simultaneamente, mas podem tentar acessar a conexão sem yield entre a checagem e a marcação de estado, por isso o lock para garantir atomicidade naquele ponto crítico. No geral, corrotinas cooperativas não precisam de locks para evitar condições de corrida de memória (já que só uma roda por vez), mas precisam para evitar *lógicas* intercaladas indevidamente.

* **Fechamento de Conexão:** Quando decidimos fechar (por erro ou policy), chamamos `await stream.close()` que, conforme implementado, remove do epoll e fecha o socket. Precisamos então garantir que qualquer future pendente seja cancelada, o estado marcado CLOSED, etc. Em `HTTP11Connection.close`, podemos sinalizar uma flag para que novas requisições recusem (`ConnectionNotAvailable` exceção se tentar usar após fechado).

* **Suporte a HTTP/2 e WebSockets (Evolução):** Mantendo o loop e `NetworkStream` genéricos, podemos futuramente suportar *upgrades* e multiplexação:

  * Para HTTP/2: Ao negociar via ALPN (na etapa 3) e detectar protocolo "h2", o pool vai instanciar `HTTP2Connection` em vez de HTTP1. Essa classe usará provavelmente a biblioteca `h2` (hyper-h2) para gerenciar frames. O importante é que **a mesma infra de loop e NetworkStream é reutilizável**. A diferença: no HTTP/2, múltiplas streams (requisições) são multiplexadas no mesmo socket. Ou seja, podemos ter várias corrotinas ativas lendo/escrevendo *no mesmo* `NetworkStream`. Será necessário adaptar nossa abordagem para permitir isso: talvez a `HTTP2Connection` rode uma coroutine de *leitura contínua* no socket que distribui frames para as requests pendentes. Isso é complexo, mas factível. Nosso loop epoll continua útil – ele notificará quando o socket h2 tem dados, então a tarefa de recepção de HTTP/2 acorda e processa todos frames, roteando-os internamente. No envio, poderemos ter locks por stream ou janelas, mas o loop não muda.
  * Para WebSocket: O upgrade para WebSocket ocorre via uma resposta HTTP/1.1 e depois o protocolo se torna bidirecional. Nossa estrutura de loop suporta bidirecionalidade (afinal, monitora leitura e escrita). Podemos, após upgrade, expor um novo objeto de stream para o usuário (que lê mensagens em vez de bytes, etc.), mas internamente continuar usando `NetworkStream` para bits na rede. O loop epoll não precisa distinguir – ele apenas entrega eventos.

Em conclusão, o gerenciamento de **keep-alive** fica simples e robusto com auxílio do `h11` e do nosso controle de conexão:

* **Reuse eficiente:** Conexão fica `IDLE` e reutilizável imediatamente após uma resposta se completou sem fechar.
* **Limpeza garantida:** Se algo der errado durante uma requisição (exceção de I/O, timeout, parser error), nossa implementação envolverá um bloco `try/except` em `handle_request` para fechar a conexão em caso de erro, evitando vazamento de recursos.
* **Políticas configuráveis:** Podemos parametrizar tempos de expiração, número máximo de requests por conexão, etc., facilmente, pois temos pontos centralizados onde a conexão é marcada idle ou fechada.
* **Thread-safety vs Task-safety:** Como o loop é single-thread, *thread-safety* no acesso aos sockets não é preocupação – o próprio GIL (quando não liberado) e o design cooperativo impedem condições de corrida clássicas. No entanto, marcamos claramente regiões críticas com locks para *task-safety* (e para futuras implementações multi-loop talvez). Além disso, tornamos o pool thread-safe usando locks se ele puder ser acessado por múltiplos threads (no caso de um cliente sync que usa threads para múltiplas requests simultâneas, por exemplo).

No geral, a abordagem mantém a conexão aberta o máximo possível para melhorar desempenho (menos handshakes TCP/TLS), mas também garante que recursos sejam liberados quando necessário e que não ocorram conflitos no uso dessas conexões.

## Interface Segura e Performática (Expondo Loop e Sockets ao Python)

Embora grande parte da lógica resida em Cython/C, precisamos expor uma interface Python ergonômica e segura para que desenvolvedores consumam esse *HTTP core*. Aqui abordamos práticas e convenções adotadas:

* **Abstração do Loop para o Usuário:** Idealmente, o usuário da biblioteca não interage diretamente com o loop de eventos de baixo nível. Ele usará objetos de nível mais alto como `ConnectionPool` ou métodos de conveniência (ex: `response = await pool.request(...)`). Internamente, o pool obtém ou cria uma `HTTP11Connection`, que por sua vez usa o loop. Podemos inicializar o loop automaticamente (um singleton global ou por instância de pool) para simplificar. Isso é semelhante ao `asyncio`, onde o usuário raramente instancia manualmente loops (a não ser para configurações especiais). Nossa implementação pode ter, por exemplo, `AsyncBackend = EpollBackend()` por padrão, e o pool utiliza esse backend.

  * Se necessário, podemos expor o loop via uma API tipo `with EpollEventLoop() as loop:` ou um método `loop.run(coro)`. Isso daria controle ao usuário avançado, mas não é obrigatório para uso comum.
  * Em suma, **escondemos detalhes do epoll** atrás de classes Python intuitivas (Connection, Pool, etc.), mantendo o poder de configuração (por exemplo, permitir trocar para um backend diferente se quisermos oferecer fallback para Windows com IOCP, etc., no futuro).

* **Isolamento do File Descriptor:** Nunca retornamos ou exigimos do usuário um número de FD ou um objeto *socket* cru. O `NetworkStream` encapsula isso. Isso evita que o usuário faça operações bloqueantes acidentalmente ou feche o FD por conta própria causando inconsistências. Toda interação passa pelos métodos async adequados.

* **Tratamento de Exceções e Erros:** Forneceremos exceções específicas (muitas já existem em httpcore, como `ConnectError`, `ReadTimeout`, etc.). Nosso código de baixo nível deve capturar erros de sistema (e.g. ECONNRESET, ETIMEDOUT) e traduzi-los para essas exceções Python de alto nível, enriquecendo com contexto se possível. Por exemplo, se `epoll_wait` retorna erro (raro, mas possível se configurações erradas), podemos lançar um `RuntimeError` interno informando falha no loop. Se uma operação de socket retornar ECONNRESET, podemos lançar talvez uma `RemoteProtocolError` ou similar, para sinalizar que a conexão caiu abruptamente.

  * Importante é **não deixar exceções sem tratamento** estourarem do nível C para Python sem tradução, pois isso poderia incluir códigos de erro pouco amigáveis ou mesmo corromper o estado do loop.
  * Adicionalmente, devemos considerar a possibilidade de *cancellation* (por exemplo, o usuário cancela uma tarefa de request). Precisaremos interceptar cancelamento (CancellationError) na coroutine e decidir: provavelmente fechar a conexão (porque o protocolo ficou em estado incompleto). Isso será alinhado com a semântica do `httpcore` que é task-safe.

* **Uso Correto de Memória e Ponteiros:** Ao interagir com C, cuidaremos para não deixar ponteiros soltos ou usar memória após liberada. Exemplo: se alocamos um buffer para ler dados, convertemos imediatamente para Python bytes antes de sair da função (ou usamos funções do Python/C API que já criam bytes para nós). Assim, garantimos que a vida útil dos dados é gerenciada pelo GC Python.

  * As estruturas do loop (como a lista de eventos epoll) serão alocadas uma vez e limpas no final do loop. O epoll FD será fechado no destrutor do loop (usando `PyObject_Destroy` equivalentes ou contexto gerenciador).
  * Também garantimos que referências para objetos Python dentro do loop (como futures armazenadas) sejam referenciadas corretamente (Py\_INCREF quando guardamos, Py\_DECREF quando removemos do registro após uso) para evitar uso após liberação ou memory leaks. Cython facilita parte disso, mas atenção manual é necessário para estruturas c interagindo com PyObjects.

* **Thread-Safety da Exposição:** Como mencionado, o nosso loop e conexões não são thread-safe por design (1 loop = 1 thread). Documentaremos isso e, se houver necessidade de multi-threading (por exemplo, vários loops em threads diferentes para escalar em núcleos diferentes), sugeriremos que o usuário crie múltiplas instâncias de `ConnectionPool` isoladas ou use a interface síncrona (que por sua vez pode gerir threads). O importante é evitar que duas threads chamem métodos no mesmo loop simultaneamente. Para robustez, podemos implementar verificações internas (assert current\_thread == loop\_thread).

  * Por outro lado, nossas primitives *são* naturalmente seguras entre tasks (corrotinas) – porque o acesso é sequencializado. O uso de locks internos (ex.: no pool e na conexão) lida com concorrência lógica entre tasks.

* **Garantindo Desempenho:** Cada camada adicionada (loop -> stream -> connection) foi pensada para ser o mais enxuta possível:

  * O loop notifica diretamente a task certa, sem intermediários desnecessários.
  * O `NetworkStream` faz operações de leitura/escrita diretas, sem copiar dados mais do que o necessário (por ex., não converte bytes em string ou coisas assim).
  * A integração com `h11` ocorre passando bytes brutos para o parser e obtendo bytes para enviar – operações que são essencialmente memórias contíguas sendo copiadas uma vez para/da rede.
  * **Exemplo de caminho otimizado:** Ao receber dados do socket, digamos 8 KB, nosso loop pega do kernel para um buffer e cria um objeto bytes Python de 8 KB. Esse objeto é alimentado ao `h11` que possivelmente referencia esse buffer internamente para parsear headers, etc., sem novas cópias (dependendo da implementação do h11). Quando formos enviar, `h11` nos dá bytes prontos (que podem ser parte do original ou combinados) – enviamos para o socket. Assim, evitamos overhead de montar strings ou buffers manualmente no Python a cada pedaço.

* **Logging e Depuração:** Embora não seja foco, podemos integrar pontos de log (como evento de conexão aberta, fechada, bytes enviados/recebidos) para facilitar depuração de problemas. Isso deve ser feito cuidadosamente para não afetar performance, talvez habilitado condicionalmente (e.g., via um flag de debug).

* **API de Alto Nível Imutável:** Como mencionado, `Request` e `Response` são expostos como tipos de alto nível que usuários do cliente final (como httpx) interagem. Eles não verão a existência de `NetworkStream` ou do loop – isso tudo acontece "sob o capô". Garantimos assim uma API limpa e focada apenas em funcionalidades HTTP, enquanto detalhes de transporte são encapsulados.

Resumindo esta parte: *expor de forma segura* significa que o usuário não precisa conhecer epoll, nem se preocupar com race conditions em sockets, e *expor de forma performática* significa que nossas abstrações não introduzem overhead significativo comparado ao uso direto de sockets. Conseguiremos isso aproveitando Cython para otimizar internamente, enquanto fornecemos conveniências Python externamente.

## Uso do Cython e Convenções de Otimização

A escolha de Cython para partes críticas permite otimizações próximas de C, mantendo integração com Python. Alguns pontos notáveis de como e onde utilizaremos Cython para desempenho:

* **Implementação do Loop e Streams em Cython:** Todo o loop de eventos epoll e a classe `NetworkStream` serão escritos em Cython (`.pyx`), possibilitando:

  * Declaração de variáveis C (`cdef`) para elementos como descritores de arquivo (int), flags de eventos (uint32), estruturas epoll, evitando conversões para objetos Python nos trechos críticos.
  * Funções definidas como `cdef` (não acessíveis diretamente em Python) para as rotinas do loop que precisam ser rápidas e chamadas muitas vezes, como a rotina que processa eventos retornados do epoll.
  * Uso de `with nogil` ao chamar chamadas bloqueantes do sistema: `epoll_wait`, `read`, `write`, etc., para que o GIL não seja segurado durante espera de I/O. Isso melhora a concorrência com outras threads Python no processo (por exemplo, bibliotecas de logging, ou se executado dentro de um servidor multi-thread).
  * Possibilidade de marcar funções críticas com diretivas do Cython: `@cython.boundscheck(False)` e `@cython.wraparound(False)` para eliminar checagem de limites em acessos de arrays C (por exemplo, iterando o array de eventos do epoll retornados, sabemos o tamanho exato recebido, então podemos indexar sem custo extra).
  * Inlining de funções pequenas: Cython permite `inline` para evitar overhead de chamada de função em loops intensivos, que podemos usar em helpers internos do loop.

* **Manipulação de Bytes e Buffer:** Ao receber dados, podemos alocar um buffer fixo (por ex, um array `cdef char[65536]`) e usar `read` para preencher. Depois, podemos converter para Python bytes via `PyBytes_FromStringAndSize(buffer, length)`. Isso cria um objeto bytes *sem cópia extra*, apenas encapsula o mesmo conteúdo (ou copia internamente uma vez do buffer C para a memória gerenciada do Python). Alternativamente, poderíamos usar `bytearray` pré-alocado e depois cortar. De qualquer forma, minimizamos realocações.

  * Para escrita, se recebemos um Python bytes, podemos acessar seu buffer interno via `PyBytes_AS_STRING` e `PyBytes_GET_SIZE` em C, e passar direto a esse buffer para `send` (evitando copiar para um buffer temporário). Isso requer cuidado para não modificar (imutar bytes) e assegurar que o GIL está segurado quando acessamos buffer Python ou fixar o buffer.
  * Essas técnicas garantem que não estejamos movimentando dados na memória desnecessariamente. A maior cópia inevitável é entre o kernel e o nosso processo (no read/write), mas além disso buscamos zero-cópias adicionais.

* **Conexões Persistentes e Objetos Reutilizáveis:** Podemos optar por reutilizar objetos para reduzir pressão no garbage collector:

  * Exemplo: o objeto `HTTP11Connection` e seu `h11.Connection` interno são mantidos para múltiplas requests. Nós chamamos `start_next_cycle()` para reaproveitar o mesmo parser ao invés de descartá-lo e criar outro. Isso economiza alocações e aquece caches de execução.
  * O nosso array de eventos epoll pode ser alocado uma vez (capacidade fixa) e reutilizado a cada loop, em vez de criar lista Python de eventos. Usando memória C bruta e depois iterando para acionar tasks corresponde a um padrão muito leve.

* **Pontos de Uso Intensivo de CPU:** Em geral, espera-se que o tempo de CPU maior seja gasto no parsing HTTP (que está em Python puro, `h11`), e possivelmente criptografia TLS (futuramente, mas isso usaremos `ssl` lib). Nossas partes em Cython buscam *reduzir o overhead adicional* dessas operações:

  * Agendamento de corrotinas e troca de contexto: São operações inevitáveis, mas podemos otimizá-las. Por exemplo, implementar um *task scheduler* simples em Cython evita chamadas das camadas de asyncio (que são mais gerais). Nossa troca de contexto se resume a guardar a referência da coroutine em espera e retomá-la – apenas poucas instruções em C.
  * Sincronização: O uso de locks assíncronos e futures será leve. Podemos implementar a lista de waiters de um lock como uma simples lista Python, o que é suficiente dado que raramente haverá contes de milhares de waiters. O importante é que adquirir ou liberar lock seja O(1).

* **Comparação com abordagens existentes:** Podemos citar que uvloop, escrito em Cython, adotou práticas similares – integrando com libuv (C otimizado) – e conseguiu aumentos de desempenho notáveis no throughput. Nossa implementação, focada em epoll diretamente, evita camadas genéricas do libuv, podendo ser até mais fina para casos HTTP. Ferramentas antigas como Eventlet e Shrapnel já demonstraram ganhos ao mover a espera de socket para C e usar corrotinas em vez de threads. Estamos seguindo um caminho comprovado por essas soluções, aplicando as convenções modernas (async/await com Python 3).

* **Manutenibilidade e Legibilidade:** Apesar de otimizações, manteremos o código estruturado:

  * Separar claramente código Cython (baixo nível) e Python (alto nível), com interfaces bem definidas. Ex.: `NetworkStream` pode ter métodos implementados em Cython mas uma definição em Python stub para fins de tipo.
  * Documentar as partes críticas (um comentário no Cython explicando que determinada seção está liberando GIL e por quê, etc.), para futuros desenvolvedores entenderem o racional das otimizações.
  * Seguir convenções PEP onde aplicável, e Cython best practices (como evitar criar objetos Python em loops quentes, etc.).

* **Semântica Correta vs Micro-otimização:** Priorizamos primeiro a implementação correta do protocolo e concorrência. Otimizações serão valiosas apenas se não quebrarem a semântica. Por exemplo, não adianta ser 5% mais rápido se quebrar thread-safety ou protocolo. Então seremos criteriosos: ativar otimizações somente onde ganho é claro e não altera comportamento observado.

Em suma, a utilização de Cython nos pontos certos permite obter a **velocidade do C** para a parte de vigília de I/O e manipulação de sockets, enquanto mantemos a **facilidade do Python** para interagir com lógica de protocolo (`h11`) e construir a API de alto nível. Essa combinação, quando bem projetada, oferece o melhor dos dois mundos: um sistema que é **rápido, escalável e ainda assim relativamente fácil de desenvolver e manter**.

## Conclusão e Próximos Passos

Este plano delineia a implementação da Etapa 1 – um núcleo HTTP/1.1 customizado – cobrindo desde o **loop de eventos em C com epoll** até as classes de alto nível para requisição e resposta. Recapitulando os pontos-chave deste design:

* Adotamos um loop de I/O dedicado escrito em Cython/C que minimiza latência e overhead, suportando milhares de sockets simultâneos com complexidade O(1) por evento. Ele coordena tarefas assíncronas de forma cooperativa, despertando corrotinas conforme dados chegam ou podem ser enviados, similar a loops consagrados (uvloop, Shrapnel) que provaram ganhos de 2-4x sobre abordagens puramente em Python.
* Definimos interfaces claras (`NetworkStream`, `HTTP11Connection`, etc.) separando responsabilidades: a camada de transporte cuida de sockets e sincronização, enquanto a camada de protocolo foca em HTTP (usando `h11` para robustez). Essa modularidade facilita extensão para outros protocolos (HTTP/2, WebSocket) sem refazer todo o mecanismo de I/O.
* Implementamos streaming eficiente, permitindo iniciar processamento de respostas assim que chegam os primeiros bytes, e enviar grandes requisições por partes. Tudo isso sem bloquear o loop e usando memória de forma controlada (iteradores de corpo). O design lida naturalmente com fluxos longos, tornando-o adequado tanto para pequenas requisições rápidas quanto para cenários de streaming de longa duração.
* Garantimos a correta reutilização de conexões (keep-alive) conforme o protocolo, aumentando throughput em cenários de múltiplas requisições sequenciais. Ao mesmo tempo, estabelecemos pontos de fechamento seguro de conexões quando necessário – integrando com políticas de tempo ocioso e tratamento de sinalização de encerramento.
* A preocupação com *task-safety* é evidente no uso de locks para evitar condições de disputa dentro de uma mesma conexão e no projeto do pool (pronto para usar locks ou semáforos para limitar conexões simultâneas a um host). E embora não busquemos multi-threading no loop, estamos cientes de questões de thread-safety e tomamos medidas (como GIL controlado, e potencial interface thread-safe para submissão de tarefas) para manter a robustez mesmo se o loop for usado de forma híbrida em aplicativos complexos.
* Utilizamos Cython de forma estratégica para maximizar performance: desde acessar diretamente chamadas de sistema epoll/read/write, até otimizações de memória e evitar overhead de chamadas Python em loops críticos. Tudo isso ficando transparente para quem usa a biblioteca, que verá apenas objetos Python de alto nível operando sem atrasos aparentes.
* **Evolução futura:** Com a base HTTP/1.1 sólida, as próximas etapas (conforme o plano geral) abordarão:

  * *Pool de Conexões* (Etapa 2): Gerenciando múltiplas conexões simultâneas, reutilizando e limitando conforme necessário. Nossa implementação atual já suporta múltiplas conexões no epoll; o pool será uma camada acima orquestrando isso.
  * *TLS e HTTP/2* (Etapa 3 em diante): Adicionando TLS com ALPN para negociar HTTP/2. O loop de eventos e NetworkStream serão expandidos para suportar sockets TLS (provavelmente integrando com `ssl` module do Python, que é compatível com espera não-bloqueante). Com HTTP/2, introduziremos o gerenciamento de múltiplos streams multiplexados – um desafio que nossa arquitetura modular está apta a receber, isolando a complexidade no componente `HTTP2Connection` enquanto reutiliza o mesmo loop subjacente.
  * *WebSockets e SSE:* Graças ao controle manual do loop, podemos suportar upgrades de protocolo e conexões de longa duração simultâneas eficientemente. Isso permitirá construir funcionalidades de tempo real (websocket, server-sent events) sobre o mesmo core, apenas tratando a interpretação dos dados de maneira diferente após upgrade.

Por fim, enfatizamos que cada componente será acompanhado de **testes abrangentes** e medidas de desempenho. Podemos usar servidores de eco e ferramentas como `httpbin` para validar comportamentos (GET, POST, chunked encoding, etc.), e realizar benchmarks comparativos com implementações existentes (asyncio, requests, etc.) para quantificar os ganhos. Com essa abordagem passo a passo, garantimos que a fundação implementada na Etapa 1 seja **robusta, eficiente e preparada para sustentação das funcionalidades avançadas** que virão nas próximas etapas. Esta base bem projetada nos deixa confiantes de poder construir um cliente HTTP de alta performance, competitivo com os melhores disponíveis, mas sob medida para nossas necessidades de controle e extensibilidade.
